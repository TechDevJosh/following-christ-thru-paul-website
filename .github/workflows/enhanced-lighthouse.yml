name: Enhanced SEO & Performance Audit

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run weekly performance audits
    - cron: '0 9 * * 1'

jobs:
  lighthouse-audit:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          cache-dependency-path: following-christ-thru-paul-website/package-lock.json
          
      - name: Install dependencies
        working-directory: following-christ-thru-paul-website
        run: npm ci
        
      - name: Build application
        working-directory: following-christ-thru-paul-website
        run: npm run build
        env:
          NEXT_PUBLIC_SANITY_PROJECT_ID: dmowh6jp
          NEXT_PUBLIC_SANITY_DATASET: production
          NEXT_PUBLIC_SANITY_API_VERSION: 2025-07-02
          
      - name: Start application
        working-directory: following-christ-thru-paul-website
        run: npm start &
        env:
          NEXT_PUBLIC_SANITY_PROJECT_ID: dmowh6jp
          NEXT_PUBLIC_SANITY_DATASET: production
          NEXT_PUBLIC_SANITY_API_VERSION: 2025-07-02
          
      - name: Wait for server
        run: npx wait-on http://localhost:3000
        
      - name: Run Enhanced Lighthouse CI
        uses: treosh/lighthouse-ci-action@v10
        with:
          configPath: following-christ-thru-paul-website/.lighthouserc.json
          uploadArtifacts: true
          temporaryPublicStorage: true
          
      - name: SEO Content Analysis
        working-directory: following-christ-thru-paul-website
        run: |
          echo "üîç Running SEO Content Analysis..."
          
          # Create SEO analysis script
          cat > seo-analysis.js << 'EOF'
          const fs = require('fs');
          const path = require('path');
          const glob = require('glob');
          
          console.log('üìä SEO Content Analysis Report\n');
          
          // Analyze page files for SEO compliance
          const pageFiles = glob.sync('src/app/**/page.tsx');
          let totalPages = 0;
          let pagesWithMetadata = 0;
          let pagesWithStructuredData = 0;
          let seoIssues = [];
          
          pageFiles.forEach(file => {
            totalPages++;
            const content = fs.readFileSync(file, 'utf8');
            
            // Check for metadata export
            if (content.includes('export const metadata') || content.includes('generateMetadata')) {
              pagesWithMetadata++;
            } else {
              seoIssues.push(`‚ùå ${file}: Missing metadata`);
            }
            
            // Check for structured data
            if (content.includes('StructuredData') || content.includes('application/ld+json')) {
              pagesWithStructuredData++;
            }
            
            // Check title length in metadata
            const titleMatch = content.match(/title:\s*["`']([^"`']+)["`']/);
            if (titleMatch && titleMatch[1]) {
              const titleLength = titleMatch[1].length;
              if (titleLength > 60) {
                seoIssues.push(`‚ö†Ô∏è ${file}: Title too long (${titleLength} chars)`);
              } else if (titleLength < 30) {
                seoIssues.push(`‚ö†Ô∏è ${file}: Title too short (${titleLength} chars)`);
              }
            }
            
            // Check description length
            const descMatch = content.match(/description:\s*["`']([^"`']+)["`']/);
            if (descMatch && descMatch[1]) {
              const descLength = descMatch[1].length;
              if (descLength > 156) {
                seoIssues.push(`‚ö†Ô∏è ${file}: Description too long (${descLength} chars)`);
              } else if (descLength < 120) {
                seoIssues.push(`‚ö†Ô∏è ${file}: Description too short (${descLength} chars)`);
              }
            }
          });
          
          console.log(`üìÑ Total Pages Analyzed: ${totalPages}`);
          console.log(`‚úÖ Pages with Metadata: ${pagesWithMetadata}/${totalPages} (${Math.round(pagesWithMetadata/totalPages*100)}%)`);
          console.log(`üèóÔ∏è Pages with Structured Data: ${pagesWithStructuredData}/${totalPages} (${Math.round(pagesWithStructuredData/totalPages*100)}%)`);
          
          if (seoIssues.length > 0) {
            console.log('\nüö® SEO Issues Found:');
            seoIssues.forEach(issue => console.log(issue));
          } else {
            console.log('\nüéâ No SEO issues found!');
          }
          
          // Generate summary for GitHub Actions
          const summary = {
            totalPages,
            pagesWithMetadata,
            pagesWithStructuredData,
            seoIssues: seoIssues.length,
            metadataCompliance: Math.round(pagesWithMetadata/totalPages*100),
            structuredDataCompliance: Math.round(pagesWithStructuredData/totalPages*100)
          };
          
          fs.writeFileSync('seo-summary.json', JSON.stringify(summary, null, 2));
          EOF
          
          # Install glob for the analysis
          npm install glob
          
          # Run the analysis
          node seo-analysis.js
          
      - name: Internal Link Analysis
        working-directory: following-christ-thru-paul-website
        run: |
          echo "üîó Running Internal Link Analysis..."
          
          cat > link-analysis.js << 'EOF'
          const fs = require('fs');
          const glob = require('glob');
          
          console.log('üîó Internal Link Analysis Report\n');
          
          const contentFiles = glob.sync('src/app/**/page.tsx');
          let totalLinks = 0;
          let internalLinks = 0;
          let externalLinks = 0;
          let linkIssues = [];
          
          contentFiles.forEach(file => {
            const content = fs.readFileSync(file, 'utf8');
            
            // Find all links
            const linkMatches = content.match(/<Link[^>]*href[^>]*>|<a[^>]*href[^>]*>/gi) || [];
            const internalLinkMatches = linkMatches.filter(link => 
              link.includes('href="/') || link.includes("href='/") || 
              !link.includes('http')
            );
            
            totalLinks += linkMatches.length;
            internalLinks += internalLinkMatches.length;
            externalLinks += (linkMatches.length - internalLinkMatches.length);
            
            // Check for generic anchor text
            const genericAnchors = content.match(/>(click here|read more|here|more)<\//gi) || [];
            if (genericAnchors.length > 0) {
              linkIssues.push(`‚ö†Ô∏è ${file}: ${genericAnchors.length} generic anchor text(s) found`);
            }
            
            // Word count for link density
            const wordCount = content.split(/\s+/).filter(word => word.length > 0).length;
            const linkDensity = wordCount > 0 ? (linkMatches.length / wordCount) * 100 : 0;
            
            if (linkDensity > 3) {
              linkIssues.push(`‚ö†Ô∏è ${file}: High link density (${linkDensity.toFixed(1)}%)`);
            }
          });
          
          console.log(`üîó Total Links: ${totalLinks}`);
          console.log(`üè† Internal Links: ${internalLinks} (${Math.round(internalLinks/totalLinks*100)}%)`);
          console.log(`üåê External Links: ${externalLinks} (${Math.round(externalLinks/totalLinks*100)}%)`);
          
          if (linkIssues.length > 0) {
            console.log('\nüö® Link Issues:');
            linkIssues.forEach(issue => console.log(issue));
          } else {
            console.log('\nüéâ No link issues found!');
          }
          EOF
          
          node link-analysis.js
          
      - name: Comment PR with Enhanced Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let seoSummary = {};
            try {
              seoSummary = JSON.parse(fs.readFileSync('following-christ-thru-paul-website/seo-summary.json', 'utf8'));
            } catch (e) {
              console.log('No SEO summary found');
            }
            
            const comment = `
            ## üöÄ Enhanced SEO & Performance Audit Results
            
            ### üìä SEO Compliance
            - **Metadata Coverage**: ${seoSummary.metadataCompliance || 0}% (${seoSummary.pagesWithMetadata || 0}/${seoSummary.totalPages || 0} pages)
            - **Structured Data**: ${seoSummary.structuredDataCompliance || 0}% (${seoSummary.pagesWithStructuredData || 0}/${seoSummary.totalPages || 0} pages)
            - **SEO Issues**: ${seoSummary.seoIssues || 0} found
            
            ### üéØ Performance Targets
            - ‚úÖ **Performance**: ‚â•75%
            - ‚úÖ **Accessibility**: ‚â•95%
            - ‚úÖ **Best Practices**: ‚â•90%
            - ‚úÖ **SEO**: ‚â•95%
            
            ### üîß SEO Enhancements Implemented
            - ‚úÖ **Real-time SEO Validation** in CMS Studio
            - ‚úÖ **Enhanced Structured Data** (Article, FAQ, Video, Breadcrumb)
            - ‚úÖ **Internal Link Analysis** with smart suggestions
            - ‚úÖ **Automated SEO Auditing** in CI/CD pipeline
            
            View detailed results in the [Lighthouse CI action](${context.payload.pull_request.html_url}/checks).
            
            ${seoSummary.seoIssues > 0 ? '‚ö†Ô∏è **Action Required**: SEO issues detected. Review the action logs for details.' : 'üéâ **All SEO checks passed!**'}
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });